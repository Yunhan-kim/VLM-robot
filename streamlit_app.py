import os, sys
sys.path.append(os.path.join(os.getcwd(), "GroundingDINO"))    
sys.path.append(os.path.join(os.getcwd(), "segmentation_anything"))

from utils.vision_util import VisionBase
from planners.planners import VLMPlanner

import cv2
import numpy as np
import matplotlib.cm as cm

import streamlit as st
from PIL import Image

def get_color_for_object(index):
    """
    Matlab ìŠ¤íƒ€ì¼ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ì˜ ìƒ‰ìƒì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    :param index: ê°ì²´ì˜ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
    :return: RGB ìƒ‰ìƒ (tuple)
    """
    # Matlab ìƒ‰ìƒ íŒ”ë ˆíŠ¸(tab10)ì„ ì‚¬ìš©í•˜ì—¬ ìƒ‰ìƒ ì¶”ì¶œ
    cmap = cm.get_cmap('tab10')
    color = cmap(index % 10)  # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ì˜ í¬ê¸°ì— ë§ê²Œ ì¸ë±ìŠ¤ ìˆœí™˜
    return tuple(int(c * 255) for c in color[:3])  # 0~1 ë²”ìœ„ì˜ ê°’ì„ 0~255 ë²”ìœ„ë¡œ ë³€í™˜

def draw_oriented_bbox(image, objects):
    """
    Draw oriented bounding boxes and object names on an image.
    
    :param image_path: Path to the input image.
    :param objects: List of objects, where each object is a tuple:
                    (name, logit, bbox, oriented_bbox)
                    - name: Object name (str)
                    - logit: Confidence score (float)
                    - bbox: (x_min, y_min, x_max, y_max)
                    - oriented_bbox: (cx, cy, w, h, angle)
    :return: Processed image with annotations (numpy array format).
    """
    # Load the image
    # image = Image.open(image_path)
    image = np.array(image)    

    # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    height, width = image.shape[:2]

    # í°íŠ¸ í¬ê¸°ì™€ ë‘ê»˜ë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€
    base_font_scale = max(width, height) / 1000  # ê¸°ë³¸ ë¹„ìœ¨ ì¡°ì • (ì´ë¯¸ì§€ í¬ê¸° ëŒ€ë¹„)
    base_thickness = max(1, int(base_font_scale * 2))  # ìµœì†Œ ë‘ê»˜ëŠ” 1ë¡œ ì„¤ì •

    for i, obj in enumerate(objects):
        name, logit, bbox, oriented_bbox = obj
        
        # Unpack bbox and oriented_bbox
        x_min, y_min, x_max, y_max = bbox
        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)
        (cx, cy), (w, h), angle = oriented_bbox[0]

        # Get color for the current object from the predefined palette
        color = get_color_for_object(i)

        # Draw regular bounding box
        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color, thickness=5*base_thickness)        
        
        # Add object name and logit
        text = f"{name}: {logit:.2f}"

        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        text_size = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=base_font_scale, thickness=base_thickness)[0]
        text_x, text_y = x_min - 5, y_min - 30

        # í…ìŠ¤íŠ¸ ë°°ê²½ í¬ê¸° ì„¤ì • (ì¡°ê¸ˆ ë” ì—¬ìœ ë¥¼ ì¤˜ì„œ í‚¤ì›€)
        text_w, text_h = text_size[0] + 10, text_size[1] + 10

        # í…ìŠ¤íŠ¸ ë°°ê²½ ìƒì ê·¸ë¦¬ê¸° (ê°™ì€ ìƒ‰ìƒ ì‚¬ìš©)
        cv2.rectangle(
            image, 
            (text_x, text_y - text_h), 
            (text_x + text_w, text_y + 10), 
            color,  # ë°•ìŠ¤ ìƒ‰ìƒì„ ê° ë¬¼ì²´ì˜ ìƒ‰ìƒìœ¼ë¡œ ì„¤ì •
            -1  # -1ë¡œ ì±„ì›€
        )

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ê²€ì€ìƒ‰)
        cv2.putText(
            image,
            text,
            org=(text_x + 5, text_y - 5),  # í…ìŠ¤íŠ¸ê°€ ë°•ìŠ¤ ì•ˆì— ì˜ ë“¤ì–´ê°€ê²Œ ì„¤ì •
            fontFace=cv2.FONT_HERSHEY_SIMPLEX,
            fontScale=base_font_scale,
            color=(0, 0, 0),
            thickness=base_thickness,
            lineType=cv2.LINE_AA,
        )

        # Draw oriented bounding box
        rect = cv2.boxPoints(((cx, cy), (w, h), angle))
        rect = np.intp(rect)
        cv2.polylines(image, [rect], isClosed=True, color=(255, 255, 255), thickness=4*base_thickness)

    return image

# ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜
def process_image(image_input, image):
    if image is None:
        return None    
    image_np = np.array(image)
    # image_path = 'assets/test_2.png'
    vision_base = VisionBase()
    obb_results = vision_base.obb_predict(image_input, text_prompt = "yellow.green.red.")    
    # obb_results = [['yellow', 0.8888310790061951, [2024.3189697265625, 800.58837890625, 2518.831298828125, 1355.8414306640625], [((2271.23876953125, 1085.838134765625), (522.2755126953125, 307.27642822265625), 62.840206146240234)]], ['red', 0.7884427905082703, [1533.3343505859375, 196.00796508789062, 2076.036376953125, 690.5823364257812], [((1795.741943359375, 439.44207763671875), (263.4681701660156, 536.7791137695312), 55.35905075073242)]], ['green', 0.7970165610313416, [718.8109741210938, 1089.64990234375, 1253.7325439453125, 1385.6641845703125], [((987.5362548828125, 1235.5908203125), (520.7658081054688, 258.0560607910156), 4.28381872177124)]]]    
    
    # Draw the bounding boxes on the image
    processed_image = draw_oriented_bbox(image_np, obb_results)    
    processed_image = Image.fromarray(processed_image)

    return processed_image, obb_results

# ëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜ (ì˜ˆì‹œ: VLMPlanner ì‘ë‹µ)
def chat_with_llm(history, message, env_desc):
    # env_descê°€ Noneì¸ ê²½ìš° ì¹œì ˆí•œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
    if env_desc is None:
        response_message = "The robot hasn't detected any objects yet. Please allow the robot to view objects before proceeding."
        reasoning_message = None  # Reasoning isn't available if there's no detection
    else:
        planner = VLMPlanner(
            planner_prompt_file="template",
            env_desc=env_desc
        )
        
        # Plan and reasoning generation
        plan, reasoning = planner.plan(message)
        
        # If no plan is available, inform the user
        if not plan:
            response_message = "No plan available. Please ensure the robot has detected objects and try again."
            reasoning_message = "No reasoning available."
        else:
            response_message = plan  # The actual plan is used as the response
            reasoning_message = reasoning  # Reasoning for the plan
    
    # Add the user message, bot response (plan), and reasoning to the chat history
    history.append({
        "user_message": message,
        "bot_message": response_message,
        "reasoning": reasoning_message
    })

    return history

# Streamlit ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
st.title("ğŸ¤–ğŸ“¦ VLM-Based Pick & Place ğŸ› ï¸")

with st.container():
    col1, col2 = st.columns([1, 1])
    
    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ (ì±„íŒ…)
    with col1:
        st.subheader("Chatbot")        
        
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        
        user_input = st.text_input("Type your message...")

        if st.button("Send"):
            # ì±„íŒ… ë©”ì‹œì§€ ì²˜ë¦¬ í›„ ìƒíƒœ ì—…ë°ì´íŠ¸
            st.session_state.chat_history = chat_with_llm(st.session_state.chat_history, user_input, st.session_state.env_desc)
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì§€ (Send ë²„íŠ¼ í´ë¦­ í›„)
            st.session_state.image_processing = False     
        
        # ì±„íŒ… ë‚´ì—­ ì¶œë ¥
        for entry in reversed(st.session_state.chat_history):
            st.markdown(f'<div style="background-color: #FFF9C4; padding: 10px; border-radius: 15px; margin-bottom: 10px; max-width: 80%; margin-left: auto; margin-right: 0;"><strong>You:</strong> {entry["user_message"]}</div>', unsafe_allow_html=True)
            st.markdown(f'<div style="background-color: #D1E7FF; padding: 10px; border-radius: 15px; margin-bottom: 10px; max-width: 80%;"><strong>ğŸ¤– Bot:</strong> {entry["bot_message"]}</div>', unsafe_allow_html=True)
            if entry["reasoning"]:
                st.markdown(f'<div style="background-color: #F1F1F1; padding: 10px; font-size: 12px; border-radius: 10px; margin-bottom: 10px; border: 1px solid #ddd;"><em>Reasoning:</em> {entry["reasoning"]}</div>', unsafe_allow_html=True)
            st.markdown("---")

    # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ì²˜ë¦¬)
    with col2:
        st.subheader("Upload an Image")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬
        if "image_input" not in st.session_state:
            st.session_state.image_input = None
        if "processed_image" not in st.session_state:
            st.session_state.processed_image = None  # ì²˜ìŒì—” ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ì—†ìŒ
        if "image_processing" not in st.session_state:
            st.session_state.image_processing = False  # ì´ˆê¸°ì—ëŠ” ì²˜ë¦¬ ì¤‘ì´ì§€ ì•ŠìŒ
        
        # íŒŒì¼ ì—…ë¡œë“œ
        image_input = st.file_uploader("Upload an Image", type=["png", "jpg", "jpeg"])
        image_placeholder = st.empty()  # ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ê¸° ì „ì˜ ë¹ˆ ê³µê°„ í• ë‹¹
        
        if image_input:
            # ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œëœ ê²½ìš°
            if image_input != st.session_state.image_input:
                # ì´ë¯¸ì§€ê°€ ë°”ë€Œì—ˆì„ ë•Œë§Œ ì²˜ë¦¬
                st.session_state.image_input = image_input  # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ session_stateì— ì €ì¥
                image = Image.open(image_input)  # ì´ë¯¸ì§€ ì—´ê¸°
                image_placeholder.image(image, caption="Uploaded Image", use_container_width=True)  # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                st.session_state.image_processing = True  # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘
                processed_image, obb_results = process_image(image_input, image)
                if processed_image:
                    st.session_state.processed_image = processed_image  # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥
                    st.session_state.env_desc = obb_results  # í™˜ê²½ ì„¤ëª… ì €ì¥
                    st.image(processed_image, caption="Processed Image", use_container_width=True)  # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í‘œì‹œ
                st.session_state.image_processing = False  # ì²˜ë¦¬ ì™„ë£Œ í›„ ì²˜ë¦¬ ì¤‘ì§€
            else:
                # ì´ì „ì— ì—…ë¡œë“œëœ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
                image = Image.open(st.session_state.image_input)
                image_placeholder.image(image, caption="Uploaded Image", use_container_width=True)  # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
                
                # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
                if st.session_state.processed_image:
                    st.image(st.session_state.processed_image, caption="Processed Image", use_container_width=True)  # ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í‘œì‹œ
        else:
            # ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
            image_placeholder.text("No image uploaded yet.")
            st.session_state.env_desc = None